# Stacking model: Simple logistic regression seems to works best, better than xgboost, and vastly better than just averaging
model.ensemble <- data.frame(first=pred.xgb$leave,second=pred.xgb.2$leave,third=pred.rf$leave,fourth=pred.glm$leave) # combine to dataframe
lr.optimal <- expand.grid(lambda=0.1,cp="bic")
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "plr",trControl = fitControl, tuneGrid= lr.optimal,metric="ROC")
#-----------------------------------------MODEL ASSESSMENT-----------------------------------------------------------------------------
# Predicting on the validation set with the base models
pred.xgb <- predict(xgbFit,validation, type="prob")["leave"]
pred.xgb.2 <- predict(xgbFit.2,validation, type="prob")["leave"]
pred.rf <- predict(rfFit,validation, type="prob")["leave"]
pred.glm <- predict(glmFit,validation, type="prob")["leave"]
model.ensemble <- data.frame(first=pred.xgb$leave,second=pred.xgb.2$leave,third=pred.rf$leave,fourth=pred.glm$leave) # combine to dataframe
# Logistic regression stack gives the following results
pred.stack.lr <- predict(lrFit.stacked,model.ensemble, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
# Comparing to a single xgboost model
xgb.single <- predict(xgbFit,validation, type="prob")["leave"]
performance(xgb.single$leave,validation$churn)
#-------------------------------------- TRAINING THE STACKING MODEL---------------------------------------------------------
# Stacking model: Simple logistic regression seems to works best, better than xgboost, and vastly better than just averaging
model.ensemble <- data.frame(first=pred.xgb$leave,second=pred.xgb.2$leave,third=pred.rf$leave,fourth=pred.glm$leave) # combine to dataframe
lr.optimal <- expand.grid(lambda=0.1,cp="bic")
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "glm", tuneGrid= lr.optimal,metric="ROC")
#-----------------------------------------MODEL ASSESSMENT-----------------------------------------------------------------------------
# Predicting on the validation set with the base models
pred.xgb <- predict(xgbFit,validation, type="prob")["leave"]
pred.xgb.2 <- predict(xgbFit.2,validation, type="prob")["leave"]
pred.rf <- predict(rfFit,validation, type="prob")["leave"]
pred.glm <- predict(glmFit,validation, type="prob")["leave"]
model.ensemble <- data.frame(first=pred.xgb$leave,second=pred.xgb.2$leave,third=pred.rf$leave,fourth=pred.glm$leave) # combine to dataframe
# Logistic regression stack gives the following results
pred.stack.lr <- predict(lrFit.stacked,model.ensemble, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
# Comparing to a single xgboost model
xgb.single <- predict(xgbFit,validation, type="prob")["leave"]
performance(xgb.single$leave,validation$churn)
# model glm
# model glm
glmFit <- train(churn ~ ., data = training,method = "glm",trControl = fitControl,verbose=TRUE, metric="ROC")
pred.glm <- predict(glmFit,stacked, type="prob")["leave"]
#-------------------------------------- TRAINING THE STACKING MODEL---------------------------------------------------------
# Stacking model: Simple logistic regression seems to works best, better than xgboost, and vastly better than just averaging
model.ensemble <- data.frame(first=pred.xgb$leave,second=pred.xgb.2$leave,third=pred.rf$leave,fourth=pred.glm$leave) # combine to dataframe
lr.optimal <- expand.grid(lambda=0.1,cp="bic")
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "glm", tuneGrid= lr.optimal,metric="ROC")
#-----------------------------------------MODEL ASSESSMENT-----------------------------------------------------------------------------
# Predicting on the validation set with the base models
pred.xgb <- predict(xgbFit,validation, type="prob")["leave"]
pred.xgb.2 <- predict(xgbFit.2,validation, type="prob")["leave"]
pred.rf <- predict(rfFit,validation, type="prob")["leave"]
pred.glm <- predict(glmFit,validation, type="prob")["leave"]
model.ensemble <- data.frame(first=pred.xgb$leave,second=pred.xgb.2$leave,third=pred.rf$leave,fourth=pred.glm$leave) # combine to dataframe
# Logistic regression stack gives the following results
pred.stack.lr <- predict(lrFit.stacked,model.ensemble, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
# Comparing to a single xgboost model
xgb.single <- predict(xgbFit,validation, type="prob")["leave"]
performance(xgb.single$leave,validation$churn)
# model glm
# model glm
glmFit <- train(churn ~ ., data = training,method = "glm",trControl = fitControl, metric="ROC")
pred.glm <- predict(glmFit,stacked, type="prob")["leave"]
glmFit
#-------------------------------------- TRAINING THE STACKING MODEL---------------------------------------------------------
# Stacking model: Simple logistic regression seems to works best, better than xgboost, and vastly better than just averaging
model.ensemble <- data.frame(first=pred.xgb$leave,second=pred.xgb.2$leave,third=pred.rf$leave,fourth=pred.glm$leave) # combine to dataframe
lr.optimal <- expand.grid(lambda=0.1,cp="bic")
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "glm", tuneGrid= lr.optimal,metric="ROC")
#-----------------------------------------MODEL ASSESSMENT-----------------------------------------------------------------------------
# Predicting on the validation set with the base models
pred.xgb <- predict(xgbFit,validation, type="prob")["leave"]
pred.xgb.2 <- predict(xgbFit.2,validation, type="prob")["leave"]
pred.rf <- predict(rfFit,validation, type="prob")["leave"]
pred.glm <- predict(glmFit,validation, type="prob")["leave"]
model.ensemble <- data.frame(first=pred.xgb$leave,second=pred.xgb.2$leave,third=pred.rf$leave,fourth=pred.glm$leave) # combine to dataframe
# Logistic regression stack gives the following results
pred.stack.lr <- predict(lrFit.stacked,model.ensemble, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
# Comparing to a single xgboost model
xgb.single <- predict(xgbFit,validation, type="prob")["leave"]
performance(xgb.single$leave,validation$churn)
pred.xgb <- predict(xgbFit,stacked, type="prob")["leave"]
# Stacking model: Simple logistic regression seems to works best, better than xgboost, and vastly better than just averaging
model.ensemble <- data.frame(first=pred.xgb$leave,second=pred.xgb.2$leave,third=pred.rf$leave,fourth=pred.glm$leave) # combine to dataframe
lr.optimal <- expand.grid(lambda=0.1,cp="bic")
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "glm", tuneGrid= lr.optimal,metric="ROC")
pred.xgb.stacked <- predict(xgbFit,stacked, type="prob")["leave"]
pred.rf.stacked <- predict(rfFit,stacked, type="prob")["leave"]
pred.xgb.2.stacked <- predict(xgbFit.2,stacked, type="prob")["leave"]
pred.glm.stacked <- predict(glmFit,stacked, type="prob")["leave"]
# Stacking model: Simple logistic regression seems to works best, better than xgboost, and vastly better than just averaging
model.ensemble <- data.frame(first=pred.xgb.stacked$leave,second=pred.xgb.2.stacked$leave,third=pred.rf.stacked$leave,fourth=pred.glm.stacked$leave) # combine to dataframe
lr.optimal <- expand.grid(lambda=0.1,cp="bic")
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "plt", tuneGrid= lr.optimal,metric="ROC")
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "plr", tuneGrid= lr.optimal,metric="ROC")
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "plr",trControl = fitControl, tuneGrid= lr.optimal,metric="ROC")
# Predicting on the validation set with the base models
pred.xgb <- predict(xgbFit,validation, type="prob")["leave"]
pred.xgb.2 <- predict(xgbFit.2,validation, type="prob")["leave"]
pred.rf <- predict(rfFit,validation, type="prob")["leave"]
pred.glm <- predict(glmFit,validation, type="prob")["leave"]
model.ensemble <- data.frame(first=pred.xgb$leave,second=pred.xgb.2$leave,third=pred.rf$leave,fourth=pred.glm$leave) # combine to dataframe
# Logistic regression stack gives the following results
pred.stack.lr <- predict(lrFit.stacked,model.ensemble, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
# Comparing to a single xgboost model
xgb.single <- predict(xgbFit,validation, type="prob")["leave"]
performance(xgb.single$leave,validation$churn)
#-------------------------------------- TRAINING THE STACKING MODEL---------------------------------------------------------
# Stacking model: Simple logistic regression seems to works best, better than xgboost, and vastly better than just averaging
model.ensemble <- data.frame(first=pred.xgb.stacked$leave,second=pred.xgb.2.stacked$leave,third=pred.rf.stacked$leave,fourth=pred.glm.stacked$leave) # combine to dataframe
lr.optimal <- expand.grid(lambda=0.1,cp="bic")
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "glm",trControl = fitControl, tuneGrid= lr.optimal,metric="ROC")
#-----------------------------------------MODEL ASSESSMENT-----------------------------------------------------------------------------
# Predicting on the validation set with the base models
pred.xgb <- predict(xgbFit,validation, type="prob")["leave"]
pred.xgb.2 <- predict(xgbFit.2,validation, type="prob")["leave"]
pred.rf <- predict(rfFit,validation, type="prob")["leave"]
pred.glm <- predict(glmFit,validation, type="prob")["leave"]
model.ensemble <- data.frame(first=pred.xgb$leave,second=pred.xgb.2$leave,third=pred.rf$leave,fourth=pred.glm$leave) # combine to dataframe
# Logistic regression stack gives the following results
pred.stack.lr <- predict(lrFit.stacked,model.ensemble, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
# Comparing to a single xgboost model
xgb.single <- predict(xgbFit,validation, type="prob")["leave"]
performance(xgb.single$leave,validation$churn)
# Stacking model: Simple logistic regression seems to works best, better than xgboost, and vastly better than just averaging
model.ensemble <- data.frame(first=pred.xgb.stacked$leave,second=pred.xgb.2.stacked$leave,third=pred.rf.stacked$leave,fourth=pred.glm.stacked$leave) # combine to dataframe
lr.optimal <- expand.grid(lambda=0.1,cp="bic")
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "glm",trControl = fitControl,metric="ROC")
#-----------------------------------------MODEL ASSESSMENT-----------------------------------------------------------------------------
# Predicting on the validation set with the base models
pred.xgb <- predict(xgbFit,validation, type="prob")["leave"]
pred.xgb.2 <- predict(xgbFit.2,validation, type="prob")["leave"]
pred.rf <- predict(rfFit,validation, type="prob")["leave"]
pred.glm <- predict(glmFit,validation, type="prob")["leave"]
model.ensemble <- data.frame(first=pred.xgb$leave,second=pred.xgb.2$leave,third=pred.rf$leave,fourth=pred.glm$leave) # combine to dataframe
# Logistic regression stack gives the following results
pred.stack.lr <- predict(lrFit.stacked,model.ensemble, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
# Comparing to a single xgboost model
xgb.single <- predict(xgbFit,validation, type="prob")["leave"]
performance(xgb.single$leave,validation$churn)
model.ensemble <- data.frame() # combine to dataframe
model.ensemble$pred.xgb.stacked <- predict(xgbFit,stacked, type="prob")["leave"]
?data.frame
model.ensemble <- data.frame(pred.xgb.stacked)
model.ensemble$pred.rf.stacked <- predict(rfFit,stacked, type="prob")["leave"]
model.ensemble$pred.xgb.2.stacked <- predict(xgbFit.2,stacked, type="prob")["leave"]
model.ensemble$pred.glm.stacked <- predict(glmFit,stacked, type="prob")["leave"]
# Predicting on the validation set with the base models
pred.xgb <- predict(xgbFit,validation, type="prob")["leave"]
model.ensemble <- data.frame(pred.xgb) # combine to dataframe
model.ensemble$pred.xgb.2 <- predict(xgbFit.2,validation, type="prob")["leave"]
model.ensemble$pred.rf <- predict(rfFit,validation, type="prob")["leave"]
model.ensemble$pred.glm <- predict(glmFit,validation, type="prob")["leave"]
# glm regression stack gives the following results
pred.stack.lr <- predict(lrFit.stacked,model.ensemble, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
# Comparing to a single xgboost model
xgb.single <- predict(xgbFit,validation, type="prob")["leave"]
performance(xgb.single$leave,validation$churn)
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "glm",trControl = fitControl,metric="ROC")
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "glm",trControl = fitControl,metric="ROC")
View(model.ensemble)
model.ensemble <- data.frame(pred.xgb.stacked=pred.xgb.stacked)
model.ensemble$pred.rf.stacked <- predict(rfFit,stacked, type="prob")["leave"]
model.ensemble$pred.xgb.2.stacked <- predict(xgbFit.2,stacked, type="prob")["leave"]
model.ensemble$pred.glm.stacked <- predict(glmFit,stacked, type="prob")["leave"]
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "glm",trControl = fitControl,metric="ROC")
model.ensemble$pred.rf.stacked <- predict(rfFit,stacked, type="prob")["leave"]$leave
model.ensemble <- data.frame(pred.xgb.stacked=pred.xgb.stacked$leave)
model.ensemble$pred.rf.stacked <- predict(rfFit,stacked, type="prob")["leave"]$leave
model.ensemble <- data.frame(pred.xgb.stacked=pred.xgb.stacked$leave)
model.ensemble$pred.rf.stacked <- predict(rfFit,stacked, type="prob")["leave"]$leave
model.ensemble$pred.xgb.2.stacked <- predict(xgbFit.2,stacked, type="prob")["leave"]$leave
model.ensemble$pred.glm.stacked <- predict(glmFit,stacked, type="prob")["leave"]$leave
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "glm",trControl = fitControl,metric="ROC")
# Predicting on the validation set with the base models
pred.xgb <- predict(xgbFit,validation, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb) # combine to dataframe
model.ensemble.val$pred.xgb.2 <- predict(xgbFit.2,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.rf <- predict(rfFit,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.glm <- predict(glmFit,validation, type="prob")["leave"]$leave
# glm regression stack gives the following results
pred.stack.lr <- predict(lrFit.stacked,model.ensemble.val, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
# Comparing to a single xgboost model
xgb.single <- predict(xgbFit,validation, type="prob")["leave"]
performance(xgb.single$leave,validation$churn)
# Predicting on the validation set with the base models
pred.xgb <- predict(xgbFit,validation, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb) # combine to dataframe
model.ensemble.val$pred.xgb.2 <- predict(xgbFit.2,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.rf <- predict(rfFit,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.glm <- predict(glmFit,validation, type="prob")["leave"]$leave
# glm regression stack gives the following results
pred.stack.lr <- predict(lrFit.stacked,model.ensemble.val, type="prob")["leave"]
# Predicting on the validation set with the base models
pred.xgb.val <- predict(xgbFit,validation, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb) # combine to dataframe
model.ensemble.val$pred.xgb.2.val <- predict(xgbFit.2,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.rf.val <- predict(rfFit,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.glm.val <- predict(glmFit,validation, type="prob")["leave"]$leave
# glm regression stack gives the following results
pred.stack.lr <- predict(lrFit.stacked,model.ensemble.val, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
pred.xgb.val <- predict(xgbFit,validation, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb) # combine to dataframe
model.ensemble.val$pred.xgb.2.val <- predict(xgbFit.2,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.rf.val <- predict(rfFit,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.glm.val <- predict(glmFit,validation, type="prob")["leave"]$leave
# glm regression stack gives the following results
pred.stack.lr <- predict(lrFit.stacked,model.ensemble.val, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
#-----------------------------------------MODEL ASSESSMENT-----------------------------------------------------------------------------
# Predicting on the validation set with the base models
pred.xgb.val <- predict(xgbFit,validation, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb.val) # combine to dataframe
model.ensemble.val$pred.xgb.2.val <- predict(xgbFit.2,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.rf.val <- predict(rfFit,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.glm.val <- predict(glmFit,validation, type="prob")["leave"]$leave
# glm regression stack gives the following results
pred.stack.lr <- predict(lrFit.stacked,model.ensemble.val, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
save(glmFit,list=c("glmFit"),file="results/glmModel")
#-------------------------------------- TRAINING THE STACKING MODEL---------------------------------------------------------
# Creating predictions
pred.xgb.stacked <- predict(xgbFit,stacked, type="prob")["leave"]
model.ensemble <- data.frame(pred.xgb.stacked=pred.xgb.stacked$leave)
model.ensemble$pred.rf.stacked <- predict(rfFit,stacked, type="prob")["leave"]$leave
model.ensemble$pred.xgb.2.stacked <- predict(xgbFit.2,stacked, type="prob")["leave"]$leave
model.ensemble$pred.glm.stacked <- predict(glmFit,stacked, type="prob")["leave"]$leave
# Stacking model: Simple glm regression seems to works best, better than xgboost, and vastly better than just averaging
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "glm",trControl = fitControl,metric="ROC")
pred.xgb.val <- predict(xgbFit,validation, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb.val) # combine to dataframe
model.ensemble.val$pred.xgb.2.val <- predict(xgbFit.2,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.rf.val <- predict(rfFit,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.glm.val <- predict(glmFit,validation, type="prob")["leave"]$leave
# glm regression stack gives the following results
pred.stack.lr <- predict(lrFit.stacked,model.ensemble.val, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
?save
save(xgb2Fit,list=c("xgb2Fit"),file="results/xgb2Model")
xgbFit2 <- xgbFit.2
save(xgb2Fit,list=c("xgb2Fit"),file="results/xgb2Model")
xgb2Fit <- xgbFit.2
save(xgb2Fit,list=c("xgb2Fit"),file="results/xgb2Model")
# Load the data
dataset <- read.csv2("data/training.csv",sep=",", stringsAsFactors = FALSE)
# Functions for data prep
source("encodeFeatures.R")
source("performance.R")
# Encode data as dummies and numeric
names(dataset) <- toupper(names(dataset))
dataset <- encodeFeatures(dataset)
# All the numerical features from the spreadsheet. All usage based
numerical_features <-c("ADJMOU","ADJQTY","ADJREV","ATTEMPT_MEAN","ATTEMPT_RANGE","AVG3MOU","AVG3QTY","AVG3REV","AVG6MOU","AVG6QTY","AVG6REV","AVGMOU","AVGQTY","AVGREV","BLCK_DAT_MEAN","BLCK_DAT_RANGE","BLCK_VCE_MEAN","BLCK_VCE_RANGE","CALLFWDV_MEAN","CALLFWDV_RANGE","CALLWAIT_MEAN","CALLWAIT_RANGE","CC_MOU_MEAN","CC_MOU_RANGE","CCRNDMOU_MEAN","CCRNDMOU_RANGE","CHANGE_MOU","CHANGE_REV","COMP_DAT_MEAN","COMP_DAT_RANGE","COMP_VCE_MEAN","COMP_VCE_RANGE","COMPLETE_MEAN","COMPLETE_RANGE","CUSTCARE_MEAN","CUSTCARE_RANGE","DA_MEAN","DA_RANGE","DATOVR_MEAN","DATOVR_RANGE","DROP_BLK_MEAN","DROP_BLK_RANGE","DROP_DAT_MEAN","DROP_DAT_RANGE","DROP_VCE_MEAN","DROP_VCE_RANGE","EQPDAYS","INONEMIN_MEAN","INONEMIN_RANGE","IWYLIS_VCE_MEAN","IWYLIS_VCE_RANGE","MONTHS","MOU_CDAT_MEAN","MOU_CDAT_RANGE","MOU_CVCE_MEAN","MOU_CVCE_RANGE","MOU_MEAN","MOU_OPKD_MEAN","MOU_OPKD_RANGE","MOU_OPKV_MEAN","MOU_OPKV_RANGE","MOU_PEAD_MEAN","MOU_PEAD_RANGE","MOU_PEAV_MEAN","MOU_PEAV_RANGE","MOU_RANGE","MOU_RVCE_MEAN","MOU_RVCE_RANGE","MOUIWYLISV_MEAN","MOUIWYLISV_RANGE","MOUOWYLISV_MEAN","MOUOWYLISV_RANGE","OWYLIS_VCE_MEAN","OWYLIS_VCE_RANGE","OPK_DAT_MEAN","OPK_DAT_RANGE","OPK_VCE_MEAN","OPK_VCE_RANGE","OVRMOU_MEAN","OVRMOU_RANGE","OVRREV_MEAN","OVRREV_RANGE","PEAK_DAT_MEAN","PEAK_DAT_RANGE","PEAK_VCE_MEAN","PEAK_VCE_RANGE","PLCD_DAT_MEAN","PLCD_DAT_RANGE","PLCD_VCE_MEAN","PLCD_VCE_RANGE","RECV_SMS_MEAN","RECV_SMS_RANGE","RECV_VCE_MEAN","RECV_VCE_RANGE","RETDAYS","REV_MEAN","REV_RANGE","RMCALLS","RMMOU","RMREV","ROAM_MEAN","ROAM_RANGE","THREEWAY_MEAN","THREEWAY_RANGE","TOTCALLS","TOTMOU","TOTMRC_MEAN","TOTMRC_RANGE","TOTREV","UNAN_DAT_MEAN","UNAN_DAT_RANGE","UNAN_VCE_MEAN","UNAN_VCE_RANGE","VCEOVR_MEAN","VCEOVR_RANGE")
# impute the data
source('imputeData.R')
#select features
source("selectFeatures.R")
# Loading trained sets
load("results/xgbFit")
load("results/xgb2Fit")
load("results/glmFit")
load("results/rfFit")
load("results/xgb2Fit")
usedDataset <- dataset.final#[sample(c(1:nrow(dataset.final)),round(nrow(dataset.final))*0.4),]
index <- c(1:nrow(usedDataset))
trainingData <- sample(index,round(nrow(usedDataset))*0.7)
training <- usedDataset[trainingData,]
test <- usedDataset[-trainingData,]
# Split the test set into stacked and validation
index <- c(1:nrow(test))
testData <- sample(index,round(nrow(test))*0.7)
stacked <- test[testData,]
validation <- test[-testData,]
# delete the temp datasets
remove(trainingData)
remove(usedDataset)
# Creating predictions
pred.xgb.stacked <- predict(xgbFit,stacked, type="prob")["leave"]
model.ensemble <- data.frame(pred.xgb.stacked=pred.xgb.stacked$leave)
model.ensemble$pred.rf.stacked <- predict(rfFit,stacked, type="prob")["leave"]$leave
model.ensemble$pred.xgb.2.stacked <- predict(xgb2Fit,stacked, type="prob")["leave"]$leave
model.ensemble$pred.glm.stacked <- predict(glmFit,stacked, type="prob")["leave"]$leave
# Stacking model: Simple glm regression seems to works best, better than xgboost, and vastly better than just averaging
lrFit.stacked <- train(stacked$churn ~ ., data = model.ensemble,method = "glm",trControl = fitControl,metric="ROC")
pred.xgb.stacked <- predict(xgbFit,stacked, type="prob")["leave"]
str(dataset.final)
model.ensemble$pred.glm.stacked <- predict(glmFit,stacked, type="prob")["leave"]$leave
glmFit <- train(churn ~ ., data = training,method = "glm",trControl = fitControl, metric="ROC")
fitControl <- trainControl(method = "cv",number = 5, allowParallel=T, summaryFunction = twoClassSummary,classProbs = TRUE)
set.seed(800)
glmFit <- train(churn ~ ., data = training,method = "glm",trControl = fitControl, metric="ROC")
model.ensemble$pred.glm.stacked <- predict(glmFit,stacked, type="prob")["leave"]$leave
pred.glm.stacked <- predict(glmFit,stacked, type="prob")["leave"]$leave
# model xgboost
xgb.grid <- expand.grid(nrounds = c(250,500), eta = c(0.03,0.05), max_depth = c(6), colsample_bytree = c(0.8), min_child_weight = 1, gamma=0)
xgb.optimal <- expand.grid(nrounds = 500,eta = 0.03,max_depth = 6, colsample_bytree = 0.8, min_child_weight = 1,gamma=0)
xgbFit <- train(churn ~ ., data = training,method = "xgbTree",trControl = fitControl,tuneGrid= xgb.optimal,verbose=TRUE, metric="ROC")
# model random forest.
rf.grid <- expand.grid(mtry=c(2,8,16,30))
rf.optimal <- expand.grid(mtry=30)
rfFit <- train(churn ~ ., data = training, method = "rf", trControl = fitControl, tuneGrid= rf.optimal,verbose=TRUE, metric="ROC")
# model second xgboost model
xgb.grid.2 <- expand.grid(nrounds = c(250,500), eta = c(0.03,0.05), max_depth = c(6), colsample_bytree = c(0.8), min_child_weight = 4, gamma=0)
xgb.optimal.2 <- expand.grid(nrounds = 250,eta = 0.01,max_depth = 4, colsample_bytree = 0.9, min_child_weight = 0.,gamma=0)
xgb2Fit <- train(churn ~ ., data = training,method = "xgbTree",trControl = fitControl,tuneGrid= xgb.optimal.2,verbose=TRUE, metric="ROC")
# model glm
glmFit <- train(churn ~ ., data = training,method = "glm",trControl = fitControl, metric="ROC")
pred.xgb.stacked <- predict(xgbFit,stacked, type="prob")["leave"]
model.ensemble <- data.frame(pred.xgb.stacked=pred.xgb.stacked$leave)
model.ensemble$pred.rf.stacked <- predict(rfFit,stacked, type="prob")["leave"]$leave
model.ensemble$pred.xgb.2.stacked <- predict(xgb2Fit,stacked, type="prob")["leave"]$leave
model.ensemble$pred.glm.stacked <- predict(glmFit,stacked, type="prob")["leave"]$leave
# Stacking model: Simple glm regression seems to works best, better than xgboost, and vastly better than just averaging
lrFit <- train(stacked$churn ~ ., data = model.ensemble,method = "plr",trControl = fitControl,metric="ROC")
# Predicting on the validation set with the base models
pred.xgb.val <- predict(xgbFit,validation, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb.val) # combine to dataframe
model.ensemble.val$pred.xgb.2.val <- predict(xgb2Fit,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.rf.val <- predict(rfFit,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.glm.val <- predict(glmFit,validation, type="prob")["leave"]$leave
# glm regression stack gives the following results
pred.stack.lr <- predict(lrFit,model.ensemble.val, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
lrFIt
lrFIt
lrFit
summary(lrFit)
pred.stack.lr <- predict(lrFit,model.ensemble.val, type="prob")["leave"]
View(model.ensemble.val)
#-------------------------------------- TRAINING THE STACKING MODEL---------------------------------------------------------
# Creating predictions
pred.xgb.stacked <- predict(xgbFit,stacked, type="prob")["leave"]
model.ensemble <- data.frame(pred.xgb=pred.xgb.stacked$leave)
model.ensemble$pred.rf <- predict(rfFit,stacked, type="prob")["leave"]$leave
model.ensemble$pred.xgb2 <- predict(xgb2Fit,stacked, type="prob")["leave"]$leave
model.ensemble$pred.glm <- predict(glmFit,stacked, type="prob")["leave"]$leave
# Stacking model: Simple glm regression seems to works best, better than xgboost, and vastly better than just averaging
lrFit <- train(stacked$churn ~ ., data = model.ensemble,method = "plr",trControl = fitControl,metric="ROC")
# Predicting on the validation set with the base models
pred.xgb.val <- predict(xgbFit,validation, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb) # combine to dataframe
model.ensemble.val$pred.xgb2 <- predict(xgb2Fit,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.rf <- predict(rfFit,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.glm <- predict(glmFit,validation, type="prob")["leave"]$leave
# glm regression stack gives the following results
pred.stack.lr <- predict(lrFit,model.ensemble.val, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
# Predicting on the validation set with the base models
pred.xgb <- predict(xgbFit,validation, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb) # combine to dataframe
model.ensemble.val$pred.xgb2 <- predict(xgb2Fit,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.rf <- predict(rfFit,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.glm <- predict(glmFit,validation, type="prob")["leave"]$leave
# glm regression stack gives the following results
pred.stack.lr <- predict(lrFit,model.ensemble.val, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
xgb.single <- predict(xgbFit,validation, type="prob")["leave"]
performance(xgb.single$leave,validation$churn)
pred.xgb.stacked <- predict(xgbFit,stacked, type="prob")["leave"]
model.ensemble <- data.frame(pred.xgb=pred.xgb.stacked$leave)
model.ensemble$pred.rf <- predict(rfFit,stacked, type="prob")["leave"]$leave
model.ensemble$pred.glm <- predict(glmFit,stacked, type="prob")["leave"]$leave
# Stacking model: Simple glm regression seems to works best, better than xgboost, and vastly better than just averaging
lrFit <- train(stacked$churn ~ ., data = model.ensemble,method = "plr",trControl = fitControl,metric="ROC")
pred.xgb <- predict(xgbFit,validation, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb) # combine to dataframe
model.ensemble.val$pred.rf <- predict(rfFit,validation, type="prob")["leave"]$leave
model.ensemble.val$pred.glm <- predict(glmFit,validation, type="prob")["leave"]$leave
# glm regression stack gives the following results
pred.stack.lr <- predict(lrFit,model.ensemble.val, type="prob")["leave"]
performance(pred.stack.lr$leave,validation$churn)
table(validation$churn)
pred.xgb <- predict(xgbFit,dataset.final, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb) # combine to dataframe
model.ensemble.val$pred.rf <- predict(rfFit,dataset.final, type="prob")["leave"]$leave
model.ensemble.val$pred.glm <- predict(glmFit,dataset.final, type="prob")["leave"]$leave
pred.stack.lr <- predict(lrFit,model.ensemble.val, type="prob")["leave"]
performance(pred.stack.lr$leave,dataset.final$churn)
usedDataset <- dataset.final#[sample(c(1:nrow(dataset.final)),round(nrow(dataset.final))*0.4),]
index <- c(1:nrow(usedDataset))
trainingData <- sample(index,round(nrow(usedDataset))*0.9)
training <- usedDataset[trainingData,]
test <- usedDataset[-trainingData,]
# Split the test set into stacked and validation
index <- c(1:nrow(test))
testData <- sample(index,round(nrow(test))*1)
stacked <- test[testData,]
validation <- test[-testData,]
# delete the temp datasets
remove(trainingData)
remove(usedDataset)
# model xgboost
xgb.grid <- expand.grid(nrounds = c(250,500), eta = c(0.03,0.05), max_depth = c(6), colsample_bytree = c(0.8), min_child_weight = 1, gamma=0)
xgb.optimal <- expand.grid(nrounds = 500,eta = 0.03,max_depth = 6, colsample_bytree = 0.8, min_child_weight = 1,gamma=0)
xgbFit <- train(churn ~ ., data = training,method = "xgbTree",trControl = fitControl,tuneGrid= xgb.optimal,verbose=TRUE, metric="ROC")
# model random forest.
rf.grid <- expand.grid(mtry=c(2,8,16,30))
rf.optimal <- expand.grid(mtry=30)
rfFit <- train(churn ~ ., data = training, method = "rf", trControl = fitControl, tuneGrid= rf.optimal,verbose=TRUE, metric="ROC")
# model glm
glmFit <- train(churn ~ ., data = training,method = "glm",trControl = fitControl, metric="ROC")
#-------------------------------------- TRAINING THE STACKING MODEL---------------------------------------------------------
# Creating predictions
pred.xgb.stacked <- predict(xgbFit,stacked, type="prob")["leave"]
model.ensemble <- data.frame(pred.xgb=pred.xgb.stacked$leave)
model.ensemble$pred.rf <- predict(rfFit,stacked, type="prob")["leave"]$leave
model.ensemble$pred.glm <- predict(glmFit,stacked, type="prob")["leave"]$leave
# Stacking model: Simple glm regression seems to works best, better than xgboost, and vastly better than just averaging
lrFit <- train(stacked$churn ~ ., data = model.ensemble,method = "plr",trControl = fitControl,metric="ROC")
# Final submissions
dataset <- read.csv2("data/test.csv",sep=",", stringsAsFactors = FALSE)
source("encodeFeatures.R")
source("performance.R")
names(dataset) <- toupper(names(dataset))
dataset <- encodeFeatures(dataset)
numerical_features <-c("ADJMOU","ADJQTY","ADJREV","ATTEMPT_MEAN","ATTEMPT_RANGE","AVG3MOU","AVG3QTY","AVG3REV","AVG6MOU","AVG6QTY","AVG6REV","AVGMOU","AVGQTY","AVGREV","BLCK_DAT_MEAN","BLCK_DAT_RANGE","BLCK_VCE_MEAN","BLCK_VCE_RANGE","CALLFWDV_MEAN","CALLFWDV_RANGE","CALLWAIT_MEAN","CALLWAIT_RANGE","CC_MOU_MEAN","CC_MOU_RANGE","CCRNDMOU_MEAN","CCRNDMOU_RANGE","CHANGE_MOU","CHANGE_REV","COMP_DAT_MEAN","COMP_DAT_RANGE","COMP_VCE_MEAN","COMP_VCE_RANGE","COMPLETE_MEAN","COMPLETE_RANGE","CUSTCARE_MEAN","CUSTCARE_RANGE","DA_MEAN","DA_RANGE","DATOVR_MEAN","DATOVR_RANGE","DROP_BLK_MEAN","DROP_BLK_RANGE","DROP_DAT_MEAN","DROP_DAT_RANGE","DROP_VCE_MEAN","DROP_VCE_RANGE","EQPDAYS","INONEMIN_MEAN","INONEMIN_RANGE","IWYLIS_VCE_MEAN","IWYLIS_VCE_RANGE","MONTHS","MOU_CDAT_MEAN","MOU_CDAT_RANGE","MOU_CVCE_MEAN","MOU_CVCE_RANGE","MOU_MEAN","MOU_OPKD_MEAN","MOU_OPKD_RANGE","MOU_OPKV_MEAN","MOU_OPKV_RANGE","MOU_PEAD_MEAN","MOU_PEAD_RANGE","MOU_PEAV_MEAN","MOU_PEAV_RANGE","MOU_RANGE","MOU_RVCE_MEAN","MOU_RVCE_RANGE","MOUIWYLISV_MEAN","MOUIWYLISV_RANGE","MOUOWYLISV_MEAN","MOUOWYLISV_RANGE","OWYLIS_VCE_MEAN","OWYLIS_VCE_RANGE","OPK_DAT_MEAN","OPK_DAT_RANGE","OPK_VCE_MEAN","OPK_VCE_RANGE","OVRMOU_MEAN","OVRMOU_RANGE","OVRREV_MEAN","OVRREV_RANGE","PEAK_DAT_MEAN","PEAK_DAT_RANGE","PEAK_VCE_MEAN","PEAK_VCE_RANGE","PLCD_DAT_MEAN","PLCD_DAT_RANGE","PLCD_VCE_MEAN","PLCD_VCE_RANGE","RECV_SMS_MEAN","RECV_SMS_RANGE","RECV_VCE_MEAN","RECV_VCE_RANGE","RETDAYS","REV_MEAN","REV_RANGE","RMCALLS","RMMOU","RMREV","ROAM_MEAN","ROAM_RANGE","THREEWAY_MEAN","THREEWAY_RANGE","TOTCALLS","TOTMOU","TOTMRC_MEAN","TOTMRC_RANGE","TOTREV","UNAN_DAT_MEAN","UNAN_DAT_RANGE","UNAN_VCE_MEAN","UNAN_VCE_RANGE","VCEOVR_MEAN","VCEOVR_RANGE")
source('imputeData.R')
source("selectFeatures.R")
pred.xgb <- predict(xgbFit,dataset.final, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb) # combine to dataframe
model.ensemble.val$pred.rf <- predict(rfFit,dataset.final, type="prob")["leave"]$leave
model.ensemble.val$pred.glm <- predict(glmFit,dataset.final, type="prob")["leave"]$leave
pred.stack.lr <- predict(lrFit,model.ensemble.val, type="prob")["leave"]
sub <- data.frame(Customer_ID=ids,EstimatedChurnProbability=pred.stack.lr$leave)
write.table(sub,"results/final_submissions.csv",row.names=FALSE, sep=",")
save(xgbFit,list=c("xgbFit"),file="results/xgbFit")
save(rfFit,list=c("rfFit"),file="results/rfFit")
save(glmFit,list=c("glmFit"),file="results/glmFit")
save(lrFit,list=c("lrFit"),file="results/lrFit")
# Final submissions
dataset <- read.csv2("data/test.csv",sep=",", stringsAsFactors = FALSE)
source("encodeFeatures.R")
source("performance.R")
names(dataset) <- toupper(names(dataset))
dataset <- encodeFeatures(dataset)
numerical_features <-c("ADJMOU","ADJQTY","ADJREV","ATTEMPT_MEAN","ATTEMPT_RANGE","AVG3MOU","AVG3QTY","AVG3REV","AVG6MOU","AVG6QTY","AVG6REV","AVGMOU","AVGQTY","AVGREV","BLCK_DAT_MEAN","BLCK_DAT_RANGE","BLCK_VCE_MEAN","BLCK_VCE_RANGE","CALLFWDV_MEAN","CALLFWDV_RANGE","CALLWAIT_MEAN","CALLWAIT_RANGE","CC_MOU_MEAN","CC_MOU_RANGE","CCRNDMOU_MEAN","CCRNDMOU_RANGE","CHANGE_MOU","CHANGE_REV","COMP_DAT_MEAN","COMP_DAT_RANGE","COMP_VCE_MEAN","COMP_VCE_RANGE","COMPLETE_MEAN","COMPLETE_RANGE","CUSTCARE_MEAN","CUSTCARE_RANGE","DA_MEAN","DA_RANGE","DATOVR_MEAN","DATOVR_RANGE","DROP_BLK_MEAN","DROP_BLK_RANGE","DROP_DAT_MEAN","DROP_DAT_RANGE","DROP_VCE_MEAN","DROP_VCE_RANGE","EQPDAYS","INONEMIN_MEAN","INONEMIN_RANGE","IWYLIS_VCE_MEAN","IWYLIS_VCE_RANGE","MONTHS","MOU_CDAT_MEAN","MOU_CDAT_RANGE","MOU_CVCE_MEAN","MOU_CVCE_RANGE","MOU_MEAN","MOU_OPKD_MEAN","MOU_OPKD_RANGE","MOU_OPKV_MEAN","MOU_OPKV_RANGE","MOU_PEAD_MEAN","MOU_PEAD_RANGE","MOU_PEAV_MEAN","MOU_PEAV_RANGE","MOU_RANGE","MOU_RVCE_MEAN","MOU_RVCE_RANGE","MOUIWYLISV_MEAN","MOUIWYLISV_RANGE","MOUOWYLISV_MEAN","MOUOWYLISV_RANGE","OWYLIS_VCE_MEAN","OWYLIS_VCE_RANGE","OPK_DAT_MEAN","OPK_DAT_RANGE","OPK_VCE_MEAN","OPK_VCE_RANGE","OVRMOU_MEAN","OVRMOU_RANGE","OVRREV_MEAN","OVRREV_RANGE","PEAK_DAT_MEAN","PEAK_DAT_RANGE","PEAK_VCE_MEAN","PEAK_VCE_RANGE","PLCD_DAT_MEAN","PLCD_DAT_RANGE","PLCD_VCE_MEAN","PLCD_VCE_RANGE","RECV_SMS_MEAN","RECV_SMS_RANGE","RECV_VCE_MEAN","RECV_VCE_RANGE","RETDAYS","REV_MEAN","REV_RANGE","RMCALLS","RMMOU","RMREV","ROAM_MEAN","ROAM_RANGE","THREEWAY_MEAN","THREEWAY_RANGE","TOTCALLS","TOTMOU","TOTMRC_MEAN","TOTMRC_RANGE","TOTREV","UNAN_DAT_MEAN","UNAN_DAT_RANGE","UNAN_VCE_MEAN","UNAN_VCE_RANGE","VCEOVR_MEAN","VCEOVR_RANGE")
source('imputeData.R')
source("selectFeatures.R")
pred.xgb <- predict(xgbFit,dataset.final, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb) # combine to dataframe
model.ensemble.val$pred.rf <- predict(rfFit,dataset.final, type="prob")["leave"]$leave
model.ensemble.val$pred.glm <- predict(glmFit,dataset.final, type="prob")["leave"]$leave
pred.stack.lr <- predict(lrFit,model.ensemble.val, type="prob")["leave"]
sub <- data.frame(Customer_ID=ids,EstimatedChurnProbability=pred.stack.lr$leave)
write.table(sub,"results/final_submissions.csv",row.names=FALSE, sep=",")
# Final submissions
dataset <- read.csv2("data/test.csv",sep=",", stringsAsFactors = FALSE)
source("encodeFeatures.R")
source("performance.R")
names(dataset) <- toupper(names(dataset))
dataset <- encodeFeatures(dataset)
numerical_features <-c("ADJMOU","ADJQTY","ADJREV","ATTEMPT_MEAN","ATTEMPT_RANGE","AVG3MOU","AVG3QTY","AVG3REV","AVG6MOU","AVG6QTY","AVG6REV","AVGMOU","AVGQTY","AVGREV","BLCK_DAT_MEAN","BLCK_DAT_RANGE","BLCK_VCE_MEAN","BLCK_VCE_RANGE","CALLFWDV_MEAN","CALLFWDV_RANGE","CALLWAIT_MEAN","CALLWAIT_RANGE","CC_MOU_MEAN","CC_MOU_RANGE","CCRNDMOU_MEAN","CCRNDMOU_RANGE","CHANGE_MOU","CHANGE_REV","COMP_DAT_MEAN","COMP_DAT_RANGE","COMP_VCE_MEAN","COMP_VCE_RANGE","COMPLETE_MEAN","COMPLETE_RANGE","CUSTCARE_MEAN","CUSTCARE_RANGE","DA_MEAN","DA_RANGE","DATOVR_MEAN","DATOVR_RANGE","DROP_BLK_MEAN","DROP_BLK_RANGE","DROP_DAT_MEAN","DROP_DAT_RANGE","DROP_VCE_MEAN","DROP_VCE_RANGE","EQPDAYS","INONEMIN_MEAN","INONEMIN_RANGE","IWYLIS_VCE_MEAN","IWYLIS_VCE_RANGE","MONTHS","MOU_CDAT_MEAN","MOU_CDAT_RANGE","MOU_CVCE_MEAN","MOU_CVCE_RANGE","MOU_MEAN","MOU_OPKD_MEAN","MOU_OPKD_RANGE","MOU_OPKV_MEAN","MOU_OPKV_RANGE","MOU_PEAD_MEAN","MOU_PEAD_RANGE","MOU_PEAV_MEAN","MOU_PEAV_RANGE","MOU_RANGE","MOU_RVCE_MEAN","MOU_RVCE_RANGE","MOUIWYLISV_MEAN","MOUIWYLISV_RANGE","MOUOWYLISV_MEAN","MOUOWYLISV_RANGE","OWYLIS_VCE_MEAN","OWYLIS_VCE_RANGE","OPK_DAT_MEAN","OPK_DAT_RANGE","OPK_VCE_MEAN","OPK_VCE_RANGE","OVRMOU_MEAN","OVRMOU_RANGE","OVRREV_MEAN","OVRREV_RANGE","PEAK_DAT_MEAN","PEAK_DAT_RANGE","PEAK_VCE_MEAN","PEAK_VCE_RANGE","PLCD_DAT_MEAN","PLCD_DAT_RANGE","PLCD_VCE_MEAN","PLCD_VCE_RANGE","RECV_SMS_MEAN","RECV_SMS_RANGE","RECV_VCE_MEAN","RECV_VCE_RANGE","RETDAYS","REV_MEAN","REV_RANGE","RMCALLS","RMMOU","RMREV","ROAM_MEAN","ROAM_RANGE","THREEWAY_MEAN","THREEWAY_RANGE","TOTCALLS","TOTMOU","TOTMRC_MEAN","TOTMRC_RANGE","TOTREV","UNAN_DAT_MEAN","UNAN_DAT_RANGE","UNAN_VCE_MEAN","UNAN_VCE_RANGE","VCEOVR_MEAN","VCEOVR_RANGE")
source('imputeData.R')
source("selectFeatures.R")
pred.xgb <- predict(xgbFit,dataset.final, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb) # combine to dataframe
model.ensemble.val$pred.rf <- predict(rfFit,dataset.final, type="prob")["leave"]$leave
model.ensemble.val$pred.glm <- predict(glmFit,dataset.final, type="prob")["leave"]$leave
pred.stack.lr <- predict(lrFit,model.ensemble.val, type="prob")["leave"]
sub <- data.frame(Customer_ID=ids,EstimatedChurnProbability=pred.stack.lr$leave)
write.table(sub,"results/final_submissions.csv",row.names=FALSE, sep=",")
# Final submissions
dataset <- read.csv2("data/test.csv",sep=",", stringsAsFactors = FALSE)
source("encodeFeatures.R")
source("performance.R")
names(dataset) <- toupper(names(dataset))
dataset <- encodeFeatures(dataset)
numerical_features <-c("ADJMOU","ADJQTY","ADJREV","ATTEMPT_MEAN","ATTEMPT_RANGE","AVG3MOU","AVG3QTY","AVG3REV","AVG6MOU","AVG6QTY","AVG6REV","AVGMOU","AVGQTY","AVGREV","BLCK_DAT_MEAN","BLCK_DAT_RANGE","BLCK_VCE_MEAN","BLCK_VCE_RANGE","CALLFWDV_MEAN","CALLFWDV_RANGE","CALLWAIT_MEAN","CALLWAIT_RANGE","CC_MOU_MEAN","CC_MOU_RANGE","CCRNDMOU_MEAN","CCRNDMOU_RANGE","CHANGE_MOU","CHANGE_REV","COMP_DAT_MEAN","COMP_DAT_RANGE","COMP_VCE_MEAN","COMP_VCE_RANGE","COMPLETE_MEAN","COMPLETE_RANGE","CUSTCARE_MEAN","CUSTCARE_RANGE","DA_MEAN","DA_RANGE","DATOVR_MEAN","DATOVR_RANGE","DROP_BLK_MEAN","DROP_BLK_RANGE","DROP_DAT_MEAN","DROP_DAT_RANGE","DROP_VCE_MEAN","DROP_VCE_RANGE","EQPDAYS","INONEMIN_MEAN","INONEMIN_RANGE","IWYLIS_VCE_MEAN","IWYLIS_VCE_RANGE","MONTHS","MOU_CDAT_MEAN","MOU_CDAT_RANGE","MOU_CVCE_MEAN","MOU_CVCE_RANGE","MOU_MEAN","MOU_OPKD_MEAN","MOU_OPKD_RANGE","MOU_OPKV_MEAN","MOU_OPKV_RANGE","MOU_PEAD_MEAN","MOU_PEAD_RANGE","MOU_PEAV_MEAN","MOU_PEAV_RANGE","MOU_RANGE","MOU_RVCE_MEAN","MOU_RVCE_RANGE","MOUIWYLISV_MEAN","MOUIWYLISV_RANGE","MOUOWYLISV_MEAN","MOUOWYLISV_RANGE","OWYLIS_VCE_MEAN","OWYLIS_VCE_RANGE","OPK_DAT_MEAN","OPK_DAT_RANGE","OPK_VCE_MEAN","OPK_VCE_RANGE","OVRMOU_MEAN","OVRMOU_RANGE","OVRREV_MEAN","OVRREV_RANGE","PEAK_DAT_MEAN","PEAK_DAT_RANGE","PEAK_VCE_MEAN","PEAK_VCE_RANGE","PLCD_DAT_MEAN","PLCD_DAT_RANGE","PLCD_VCE_MEAN","PLCD_VCE_RANGE","RECV_SMS_MEAN","RECV_SMS_RANGE","RECV_VCE_MEAN","RECV_VCE_RANGE","RETDAYS","REV_MEAN","REV_RANGE","RMCALLS","RMMOU","RMREV","ROAM_MEAN","ROAM_RANGE","THREEWAY_MEAN","THREEWAY_RANGE","TOTCALLS","TOTMOU","TOTMRC_MEAN","TOTMRC_RANGE","TOTREV","UNAN_DAT_MEAN","UNAN_DAT_RANGE","UNAN_VCE_MEAN","UNAN_VCE_RANGE","VCEOVR_MEAN","VCEOVR_RANGE")
source('imputeData.R')
source("selectFeatures.R")
pred.xgb <- predict(xgbFit,dataset.final, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb) # combine to dataframe
model.ensemble.val$pred.rf <- predict(rfFit,dataset.final, type="prob")["leave"]$leave
model.ensemble.val$pred.glm <- predict(glmFit,dataset.final, type="prob")["leave"]$leave
pred.stack.lr <- predict(lrFit,model.ensemble.val, type="prob")["leave"]
sub <- data.frame(Customer_ID=ids,EstimatedChurnProbability=pred.stack.lr$leave)
write.table(sub,"results/final_submissions.csv",row.names=FALSE, sep=",")
View(sub)
nzv <- nearZeroVar(dataset.imputed)
# There are quite a lot of nzv predictors. This means that values of the predictor are highly skewed towards one value
# for example a lot of the dummy variables have almost 99% 0s
# We will try and delete these variables and see how the performance increases
dataset.i.nzv <- dataset.imputed[,-nzv]
# This transformation did not change the overall accuracy but increased training time
# Accuracy     Kappa       AUC  Lift 10%
#0.6350000 0.2700546 0.6864603 1.5475463
# Sadly most variables that got canned were the dummy variables we encoded by hand. Time well spent!
# A lot of the usage based features are highly correlated
# Some are just linear combinations of other features, like total number of calls
# We can try to find linear combinations with an inbuilt caret feature
linCombs <- findLinearCombos(dataset.i.nzv[,names(dataset.i.nzv) %in% numerical_features])
# Final submissions
dataset <- read.csv2("data/training.csv",sep=",", stringsAsFactors = FALSE)
source("encodeFeatures.R")
source("performance.R")
names(dataset) <- toupper(names(dataset))
dataset <- encodeFeatures(dataset)
numerical_features <-c("ADJMOU","ADJQTY","ADJREV","ATTEMPT_MEAN","ATTEMPT_RANGE","AVG3MOU","AVG3QTY","AVG3REV","AVG6MOU","AVG6QTY","AVG6REV","AVGMOU","AVGQTY","AVGREV","BLCK_DAT_MEAN","BLCK_DAT_RANGE","BLCK_VCE_MEAN","BLCK_VCE_RANGE","CALLFWDV_MEAN","CALLFWDV_RANGE","CALLWAIT_MEAN","CALLWAIT_RANGE","CC_MOU_MEAN","CC_MOU_RANGE","CCRNDMOU_MEAN","CCRNDMOU_RANGE","CHANGE_MOU","CHANGE_REV","COMP_DAT_MEAN","COMP_DAT_RANGE","COMP_VCE_MEAN","COMP_VCE_RANGE","COMPLETE_MEAN","COMPLETE_RANGE","CUSTCARE_MEAN","CUSTCARE_RANGE","DA_MEAN","DA_RANGE","DATOVR_MEAN","DATOVR_RANGE","DROP_BLK_MEAN","DROP_BLK_RANGE","DROP_DAT_MEAN","DROP_DAT_RANGE","DROP_VCE_MEAN","DROP_VCE_RANGE","EQPDAYS","INONEMIN_MEAN","INONEMIN_RANGE","IWYLIS_VCE_MEAN","IWYLIS_VCE_RANGE","MONTHS","MOU_CDAT_MEAN","MOU_CDAT_RANGE","MOU_CVCE_MEAN","MOU_CVCE_RANGE","MOU_MEAN","MOU_OPKD_MEAN","MOU_OPKD_RANGE","MOU_OPKV_MEAN","MOU_OPKV_RANGE","MOU_PEAD_MEAN","MOU_PEAD_RANGE","MOU_PEAV_MEAN","MOU_PEAV_RANGE","MOU_RANGE","MOU_RVCE_MEAN","MOU_RVCE_RANGE","MOUIWYLISV_MEAN","MOUIWYLISV_RANGE","MOUOWYLISV_MEAN","MOUOWYLISV_RANGE","OWYLIS_VCE_MEAN","OWYLIS_VCE_RANGE","OPK_DAT_MEAN","OPK_DAT_RANGE","OPK_VCE_MEAN","OPK_VCE_RANGE","OVRMOU_MEAN","OVRMOU_RANGE","OVRREV_MEAN","OVRREV_RANGE","PEAK_DAT_MEAN","PEAK_DAT_RANGE","PEAK_VCE_MEAN","PEAK_VCE_RANGE","PLCD_DAT_MEAN","PLCD_DAT_RANGE","PLCD_VCE_MEAN","PLCD_VCE_RANGE","RECV_SMS_MEAN","RECV_SMS_RANGE","RECV_VCE_MEAN","RECV_VCE_RANGE","RETDAYS","REV_MEAN","REV_RANGE","RMCALLS","RMMOU","RMREV","ROAM_MEAN","ROAM_RANGE","THREEWAY_MEAN","THREEWAY_RANGE","TOTCALLS","TOTMOU","TOTMRC_MEAN","TOTMRC_RANGE","TOTREV","UNAN_DAT_MEAN","UNAN_DAT_RANGE","UNAN_VCE_MEAN","UNAN_VCE_RANGE","VCEOVR_MEAN","VCEOVR_RANGE")
source('imputeData.R')
source("selectFeatures.R")
pred.xgb <- predict(xgbFit,dataset.final, type="prob")["leave"]$leave
model.ensemble.val <- data.frame(pred.xgb) # combine to dataframe
model.ensemble.val$pred.rf <- predict(rfFit,dataset.final, type="prob")["leave"]$leave
model.ensemble.val$pred.glm <- predict(glmFit,dataset.final, type="prob")["leave"]$leave
pred.stack.lr <- predict(lrFit,model.ensemble.val, type="prob")["leave"]
sub <- data.frame(Customer_ID=ids,EstimatedChurnProbability=pred.stack.lr$leave)
write.table(sub,"results/final_submissions_training.csv",row.names=FALSE, sep=",")
