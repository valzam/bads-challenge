performance(pred.nn.prob,test$churn)
nn.optimal <- expand.grid(size=8,decay=0.01)
nnFit <- train(churn ~ ., data = training,
method = "nnet",
trControl = fitControl,
tuneGrid= nn.optimal,
verbose=TRUE,
metric="ROC"
)
pred.nn.prob <- predict(nnFit,test, type="prob")["leave"]
performance(pred.nn.prob,test$churn)
ensemble <- data.frame(xgb=pred.xgb.prob$leave,nnet=pred.nn.prob$leave)
View(ensemble)
ensemble$combined <- apply(ensemble[,names(ensemble) %in% c("xgb,nnet")],1,mean)
?apply
ensemble$combined <- ave(ensemble$xgb,ensemble$nnet)
a  <- c(1,2,3,4,5)
mean(a)
ave(a)
mean(c(0.3775,0.4961))
performance(ensemble$combined,test$churn)
performance(pred.xgb.prob,test$churn)
# Used transformation of the dataset
usedDataset <- dataset.final[sample(c(1:nrow(dataset.final)),round(nrow(dataset.final))*0.5),]
index <- c(1:nrow(usedDataset))
trainingData <- sample(index,round(nrow(usedDataset))*0.8)
test <- usedDataset[-trainingData,]
training <- usedDataset[trainingData,]
# delete the temp datasets
remove(trainingData)
remove(usedDataset)
# One xgboost model has shown us that the variables we include are ok and can be used from now on
# However, AUC is not satisfactory. The next step is to try and build up an ensemble
# We will retain the Xgboost model as a strong base learner
# We are first testing on a smaller subset of the whole data to speed up training times and make trying out
# different things possible
xgb.grid <- expand.grid(nrounds = c(250,500), eta = c(0.03,0.05), max_depth = c(6), colsample_bytree = c(0.8), min_child_weight = 1, gamma=0)
xgb.optimal <- expand.grid(nrounds = 500,eta = 0.03,max_depth = 6, colsample_bytree = 0.8, min_child_weight = 1,gamma=0)
xgbFit <- train(churn ~ ., data = training,
method = "xgbTree",
trControl = fitControl,
tuneGrid= xgb.optimal,
verbose=TRUE,
metric="ROC"
)
pred.xgb.prob <- predict(xgbFit,test, type="prob")["leave"]
performance(pred.xgb.prob,test$churn)
# Since we converted everything to numerical features we can include a neural network
nn.grid <- expand.grid(size=c(8),decay=c(0.01,0.1))
nn.optimal <- expand.grid(size=8,decay=0.01)
nnFit <- train(churn ~ ., data = training,
method = "nnet",
trControl = fitControl,
tuneGrid= nn.optimal,
verbose=TRUE,
metric="ROC"
)
pred.nn.prob <- predict(nnFit,test, type="prob")["leave"]
performance(pred.nn.prob,test$churn)
# Assemble predictions
# We are going for a simple mean prediction
# Stacking a meta learner posses a huge problem because of possible overfitting
# We will see if it is feasible to try or if the improvement through the basic ensemble is enough
ensemble <- data.frame(xgb=pred.xgb.prob$leave,nnet=pred.nn.prob$leave)
ensemble$combined <- ave(ensemble$xgb,ensemble$nnet)
performance(ensemble$combined,test$churn)
save("xgbFit","nnFit",c("xgbFit","nnFit"))
save("xgbFit","nnFit",list = c("xgbFit","nnFit"), file = "awesomeModel")
performance(pred.xgb.prob,test$churn)
performance(pred.nn.prob,test$churn)
xgb <- predict(xgbFit,dataset.final, type="prob")["leave"]
x <- data.frame(Customer_ID=ids,EstimatedChurnProbability=xgb$leave)
write.table(x,"submis.csv",sep=",",row.names=FALSE)
xgb.optimal <- expand.grid(nrounds = 500,eta = 0.03,max_depth = 6, colsample_bytree = 0.8, min_child_weight = 1,gamma=0)
xgbFit <- train(churn ~ ., data = usedDataset,
method = "xgbTree",
trControl = fitControl,
tuneGrid= xgb.optimal,
verbose=TRUE,
metric="ROC"
)
usedDataset <- dataset.final#[sample(c(1:nrow(dataset.final)),round(nrow(dataset.final))*0.5),]
xgb.optimal <- expand.grid(nrounds = 500,eta = 0.03,max_depth = 6, colsample_bytree = 0.8, min_child_weight = 1,gamma=0)
xgbFit <- train(churn ~ ., data = usedDataset,
method = "xgbTree",
trControl = fitControl,
tuneGrid= xgb.optimal,
verbose=TRUE,
metric="ROC"
)
xgb <- predict(xgbFit,dataset.final, type="prob")["leave"]
nn <- predict(nnFit,dataset.final, type="prob")["leave"]
ensemble <- data.frame(xgb=xgb$leave,nnet=nn$leave)
ensemble$combined <- ave(ensemble$xgb,ensemble$nnet)
x <- data.frame(Customer_ID=ids,EstimatedChurnProbability=xgb$leave)
write.table(x,"submis.csv",sep=",",row.names=FALSE)
# Used transformation of the dataset
usedDataset <- dataset.final#[sample(c(1:nrow(dataset.final)),round(nrow(dataset.final))*0.5),]
index <- c(1:nrow(usedDataset))
trainingData <- sample(index,round(nrow(usedDataset))*0.8)
test <- usedDataset[-trainingData,]
training <- usedDataset[trainingData,]
# delete the temp datasets
remove(trainingData)
remove(usedDataset)
rf.grid <- expand.grid(mtry=c(2,8,16,30))
rf.optimal <- expand.grid(mtry=30)
rfFit <- train(churn ~ ., data = training,
method = "rf",
trControl = fitControl,
tuneGrid= rf.optimal,
verbose=TRUE,
metric="ROC"
)
rf.grid <- expand.grid(mtry=c(2,8,16,30))
rf.optimal <- expand.grid(mtry=30)
rfFit <- train(churn ~ ., data = training,
method = "rf",
trControl = fitControl,
tuneGrid= rf.grid,
verbose=TRUE,
metric="ROC"
)
?ave
# Used transformation of the dataset
usedDataset <- dataset.final[sample(c(1:nrow(dataset.final)),round(nrow(dataset.final))*0.3),]
index <- c(1:nrow(usedDataset))
trainingData <- sample(index,round(nrow(usedDataset))*0.8)
test <- usedDataset[-trainingData,]
training <- usedDataset[trainingData,]
# delete the temp datasets
remove(trainingData)
remove(usedDataset)
# One xgboost model has shown us that the variables we include are ok and can be used from now on
# However, AUC is not satisfactory. The next step is to try and build up an ensemble
# We will retain the Xgboost model as a strong base learner
# We are first testing on a smaller subset of the whole data to speed up training times and make trying out
# different things possible
xgb.grid <- expand.grid(nrounds = c(250,500), eta = c(0.03,0.05), max_depth = c(6), colsample_bytree = c(0.8), min_child_weight = 1, gamma=0)
xgb.optimal <- expand.grid(nrounds = 500,eta = 0.03,max_depth = 6, colsample_bytree = 0.8, min_child_weight = 1,gamma=0)
xgbFit <- train(churn ~ ., data = training,
method = "xgbTree",
trControl = fitControl,
tuneGrid= xgb.optimal,
verbose=TRUE,
metric="ROC"
)
pred.xgb.prob <- predict(xgbFit,test, type="prob")["leave"]
# Since we converted everything to numerical features we can include a neural network
# Sadly nnet performance absolutely sucks, on its own and in an ensemble
# nn.grid <- expand.grid(size=c(8),decay=c(0.01,0.1))
# nn.optimal <- expand.grid(size=8,decay=0.01)
# nnFit <- train(churn ~ ., data = training,
#                 method = "nnet",
#                 trControl = fitControl,
#                 tuneGrid= nn.optimal,
#                 verbose=TRUE,
#                 metric="ROC"
# )
#pred.nn.prob <- predict(nnFit,test, type="prob")["leave"]
#performance(pred.nn.prob,test$churn)
# Random Forest might be too similar to gradient boosting, but worth a try
rf.grid <- expand.grid(mtry=c(2,8,16,30))
rf.optimal <- expand.grid(mtry=30)
rfFit <- train(churn ~ ., data = training,
method = "rf",
trControl = fitControl,
tuneGrid= rf.grid,
verbose=TRUE,
metric="ROC"
)
pred.rf.prob <- predict(rfFit,test, type="prob")["leave"]
# Assemble predictions
performance(pred.xgb.prob,test$churn)
performance(pred.rf.prob,test$churn)
# We are going for a simple mean prediction
# Stacking a meta learner posses a huge problem because of possible overfitting
# We will see if it is feasible to try or if the improvement through the basic ensemble is enough
ensemble <- data.frame(first=pred.xgb.prob$leave,second=pred.rf.prob$leave)
ensemble$combined <- ave(ensemble$first,ensemble$second)
performance(ensemble$combined,test$churn)
rfFit
ensemble <- data.frame(xgb=pred.xgb.prob$leave,xgb2=pred.xgb.prob$leave,second=pred.rf.prob$leave)
ensemble$combined <- ave(ensemble$xgb,ensemble$xgb2,ensemble$second)
performance(ensemble$combined,test$churn)
cor(ensemble$xgb,ensemble$second)
ensemble <- data.frame(xgb=pred.xgb.prob$leave,second=pred.rf.prob$leave)
ensemble$combined <- ave(ensemble$xgb,ensemble$second)
performance(ensemble$combined,test$churn)
varImpPlot(rfFit)
varImp(rfFit)
varImp(xgbFit)
View(ensemble)
e <- cbind(ensemble,test$churn)
View(e)
e <- e[with(e,order(-combined)),]
# Used transformation of the dataset
usedDataset <- dataset.final#[sample(c(1:nrow(dataset.final)),round(nrow(dataset.final))*0.8),]
index <- c(1:nrow(usedDataset))
trainingData <- sample(index,round(nrow(usedDataset))*0.8)
test <- usedDataset[-trainingData,]
training <- usedDataset[trainingData,]
# delete the temp datasets
remove(trainingData)
remove(usedDataset)
# One xgboost model has shown us that the variables we include are ok and can be used from now on
# However, AUC is not satisfactory. The next step is to try and build up an ensemble
# We will retain the Xgboost model as a strong base learner
# We are first testing on a smaller subset of the whole data to speed up training times and make trying out
# different things possible
xgb.grid <- expand.grid(nrounds = c(250,500), eta = c(0.03,0.05), max_depth = c(6), colsample_bytree = c(0.8), min_child_weight = 1, gamma=0)
xgb.optimal <- expand.grid(nrounds = 500,eta = 0.03,max_depth = 6, colsample_bytree = 0.8, min_child_weight = 1,gamma=0)
xgbFit <- train(churn ~ ., data = training,
method = "xgbTree",
trControl = fitControl,
tuneGrid= xgb.optimal,
verbose=TRUE,
metric="ROC"
)
set.seed(800)
# 5 fold cv
fitControl <- trainControl(method = "cv",number = 5, allowParallel=T, summaryFunction = twoClassSummary,classProbs = TRUE)
# Used transformation of the dataset
usedDataset <- dataset.final#[sample(c(1:nrow(dataset.final)),round(nrow(dataset.final))*0.8),]
index <- c(1:nrow(usedDataset))
trainingData <- sample(index,round(nrow(usedDataset))*0.8)
test <- usedDataset[-trainingData,]
training <- usedDataset[trainingData,]
# delete the temp datasets
remove(trainingData)
remove(usedDataset)
# One xgboost model has shown us that the variables we include are ok and can be used from now on
# However, AUC is not satisfactory. The next step is to try and build up an ensemble
# We will retain the Xgboost model as a strong base learner
# We are first testing on a smaller subset of the whole data to speed up training times and make trying out
# different things possible
xgb.grid <- expand.grid(nrounds = c(250,500), eta = c(0.03,0.05), max_depth = c(6), colsample_bytree = c(0.8), min_child_weight = 1, gamma=0)
xgb.optimal <- expand.grid(nrounds = 500,eta = 0.03,max_depth = 6, colsample_bytree = 0.8, min_child_weight = 1,gamma=0)
xgbFit <- train(churn ~ ., data = training,
method = "xgbTree",
trControl = fitControl,
tuneGrid= xgb.optimal,
verbose=TRUE,
metric="ROC"
)
pred.xgb.prob <- predict(xgbFit,test, type="prob")["leave"]
# Since we converted everything to numerical features we can include a neural network
# Sadly nnet performance absolutely sucks, on its own and in an ensemble
# nn.grid <- expand.grid(size=c(8),decay=c(0.01,0.1))
# nn.optimal <- expand.grid(size=8,decay=0.01)
# nnFit <- train(churn ~ ., data = training,
#                 method = "nnet",
#                 trControl = fitControl,
#                 tuneGrid= nn.optimal,
#                 verbose=TRUE,
#                 metric="ROC"
# )
#pred.nn.prob <- predict(nnFit,test, type="prob")["leave"]
#performance(pred.nn.prob,test$churn)
# Random Forest might be too similar to gradient boosting, but worth a try
# Result: Random forest performs a lot better than nnet but still worse than xgboost while being highly correlated to it 0.8
# the ensemble performs worse than a single xgboost
rf.grid <- expand.grid(mtry=c(2,8,16,30))
rf.optimal <- expand.grid(mtry=30)
rfFit <- train(churn ~ ., data = training,
method = "rf",
trControl = fitControl,
tuneGrid= rf.optimal,
verbose=TRUE,
metric="ROC"
)
pred.rf.prob <- predict(rfFit,test, type="prob")["leave"]
# Assemble predictions
performance(pred.xgb.prob,test$churn)
performance(pred.rf.prob,test$churn)
# We are going for a simple mean prediction
# Stacking a meta learner posses a huge problem because of possible overfitting
# We will see if it is feasible to try or if the improvement through the basic ensemble is enough
ensemble <- data.frame(xgb=pred.xgb.prob$leave,second=pred.rf.prob$leave)
ensemble$combined <- ave(ensemble$xgb,ensemble$second)
performance(ensemble$combined,test$churn)
# Another option is to train multiple xgboost models on different parts of the data and use the predictions in an ensemble
# Either averaging or with a meta learner
index <- c(1:nrow(training))
trainingData <- sample(index,round(nrow(training))*0.6)
training.sample.1 <- training[trainingData,]
test.sample.1 <-training[-trainingData,]
xgbFit.1 <- train(churn ~ ., data = training.sample.1,method = "xgbTree",trControl = fitControl,tuneGrid= xgb.optimal,verbose=TRUE, metric="ROC")
# Another option is to train multiple xgboost models on different parts of the data and use the predictions in an ensemble
# Either averaging or with a meta learner
index <- c(1:nrow(training))
trainingData <- sample(index,round(nrow(training))*0.6)
training.sample.1 <- training[trainingData,]
test.sample.1 <-training[-trainingData,]
xgbFit.1 <- train(churn ~ ., data = training.sample.1,method = "xgbTree",trControl = fitControl,tuneGrid= xgb.optimal,verbose=TRUE, metric="ROC")
trainingData <- sample(index,round(nrow(training))*0.6)
training.sample.2 <- training[trainingData,]
test.sample.2 <- training[-trainingData,]
xgbFit.2 <- train(churn ~ ., data = training.sample.2,method = "xgbTree",trControl = fitControl,tuneGrid= xgb.optimal,verbose=TRUE, metric="ROC")
trainingData <- sample(index,round(nrow(training))*0.6)
training.sample.3 <- training[trainingData,]
test.sample.3 <- training[-trainingData,]
xgbFit.3 <- train(churn ~ ., data = training.sample.3,method = "xgbTree",trControl = fitControl,tuneGrid= xgb.optimal,verbose=TRUE, metric="ROC")
# Split the test set in 2 sets. I set for training the stacked classifier
# the second set for model assessment
testData <- sample(index,round(nrow(test))*0.8)
test.sample <- test[testData,]
validation.sample <-test[-testData,]
pred.xgb.1 <- predict(xgbFit.1,test.sample, type="prob")["leave"]
pred.xgb.2 <- predict(xgbFit.2,test.sample, type="prob")["leave"]
pred.xgb.3 <- predict(xgbFit.3,test.sample, type="prob")["leave"]
# Put them together in an ensemble and train a xgboost
xgb.ensemble <- data.frame(first=pred.xgb.1$leave,second=pred.xgb.2$leave,third=pred.xgb.3)
xgbFit.stacked <- train(test.sample$churn ~ ., data = xgb.ensemble,method = "xgbTree",trControl = fitControl,tuneGrid= xgb.optimal,verbose=TRUE, metric="ROC")
pred.stack <- predict(xgbFit.stacked,test.validation$churn, type="prob")["leave"]
performance(pred.stack$leave,validation.sample$churn)
# Put them together in an ensemble and train a xgboost
xgb.ensemble <- data.frame(first=pred.xgb.1$leave,second=pred.xgb.2$leave,third=pred.xgb.3$leave)
xgbFit.stacked <- train(test.sample$churn ~ ., data = xgb.ensemble,method = "xgbTree",trControl = fitControl,tuneGrid= xgb.optimal,verbose=TRUE, metric="ROC")
pred.stack <- predict(xgbFit.stacked,test.validation$churn, type="prob")["leave"]
performance(pred.stack$leave,validation.sample$churn)
xgb.ensemble <- data.frame(first=pred.xgb.1$leave,second=pred.xgb.2$leave,third=pred.xgb.3$leave)
xgbFit.stacked <- train(test.sample$churn ~ ., data = xgb.ensemble,method = "xgbTree",trControl = fitControl,tuneGrid= xgb.optimal,verbose=TRUE, metric="ROC")
any(is.na(test.sample))
index <- c(1:nrow(test))
testData <- sample(index,round(nrow(test))*0.8)
test.sample <- test[testData,]
validation.sample <-test[-testData,]
pred.xgb.1 <- predict(xgbFit.1,test.sample, type="prob")["leave"]
pred.xgb.2 <- predict(xgbFit.2,test.sample, type="prob")["leave"]
pred.xgb.3 <- predict(xgbFit.3,test.sample, type="prob")["leave"]
# Put them together in an ensemble and train a xgboost
xgb.ensemble <- data.frame(first=pred.xgb.1$leave,second=pred.xgb.2$leave,third=pred.xgb.3$leave)
xgbFit.stacked <- train(test.sample$churn ~ ., data = xgb.ensemble,method = "xgbTree",trControl = fitControl,tuneGrid= xgb.optimal,verbose=TRUE, metric="ROC")
pred.stack <- predict(xgbFit.stacked,test.validation$churn, type="prob")["leave"]
performance(pred.stack$leave,validation.sample$churn)
pred.stack <- predict(xgbFit.stacked,validation.sample$churn, type="prob")["leave"]
performance(pred.stack$leave,validation.sample$churn)
xgbFit.stacked <- train(test.sample$churn ~ ., data = xgb.ensemble,method = "xgbTree",trControl = fitControl,tuneGrid= xgb.optimal,verbose=TRUE, metric="ROC")
pred.stack <- predict(xgbFit.stacked,validation.sample$churn, type="prob")["leave"]
# Create predictions for the validation set
pred.xgb.1 <- predict(xgbFit.1,validation.sample, type="prob")["leave"]
pred.xgb.2 <- predict(xgbFit.2,validation.sample, type="prob")["leave"]
pred.xgb.3 <- predict(xgbFit.3,validation.sample, type="prob")["leave"]
xgb.ensemble.validation <- data.frame(first=pred.xgb.1$leave,second=pred.xgb.2$leave,third=pred.xgb.3$leave)
pred.stack <- predict(xgbFit.stacked,xgb.ensemble.validation, type="prob")["leave"]
performance(pred.stack$leave,validation.sample$churn)
xgbFit.stacked
performance(pred.xgb.prob,test$churn)
pred.xgb.prob <- predict(xgbFit,validation.sample, type="prob")["leave"]
performance(pred.xgb.prob,validation.sample$churn)
# Simply averaging the 3 xgboost models gives the following results
xgb.ensemble.validation$ave <- ave(xgb.ensemble.validation$first,xgb.ensemble.validation$second,xgb.ensemble.validation$third)
performance(xgb.ensemble.validation$ave,validation.sample$churn)
lrFit.stacked <- train(test.sample$churn ~ ., data = xgb.ensemble,method = "polr",trControl = fitControl, metric="ROC")
lrFit.stacked <- train(test.sample$churn ~ ., data = xgb.ensemble,method = "plr",trControl = fitControl, metric="ROC")
lrFit.stacked
pred.stack.lr <- predict(lrFit.stacked,xgb.ensemble.validation, type="prob")["leave"]
performance(pred.stack.lr$leave,validation.sample$churn)
summary(lrFit.stacked)
pred.xgb.1 <- predict(xgbFit.1,dataset.final, type="prob")["leave"]
pred.xgb.2 <- predict(xgbFit.2,dataset.final, type="prob")["leave"]
pred.xgb.3 <- predict(xgbFit.3,dataset.final, type="prob")["leave"]
xgb.ensemble.validation <- data.frame(first=pred.xgb.1$leave,second=pred.xgb.2$leave,third=pred.xgb.3$leave)
pred.stack.lr <- predict(lrFit.stacked,dataset.final, type="prob")["leave"]
performance(pred.stack.lr$leave,dataset.final$churn)
x <- data.frame(Customer_ID=ids,EstimatedChurnProbability=pred.stack.lr$leave)
write.table(x,"submis_lr_ensemble.csv",sep=",",row.names=FALSE)
xgb.ensemble.validation <- data.frame(first=pred.xgb.1$leave,second=pred.xgb.2$leave,third=pred.xgb.3$leave)
pred.stack.lr <- predict(lrFit.stacked,xgb.ensemble.validation, type="prob")["leave"]
performance(pred.stack.lr$leave,dataset.final$churn)
x <- data.frame(Customer_ID=ids,EstimatedChurnProbability=pred.stack.lr$leave)
write.table(x,"submis_lr_ensemble.csv",sep=",",row.names=FALSE)
pred.xgb.prob <- predict(xgbFit,dataset.final, type="prob")["leave"]
performance(pred.xgb.prob,dataset.final$churn)
x <- data.frame(Customer_ID=ids,EstimatedChurnProbability=pred.xgb.prob$leave)
write.table(x,"submis_single_xgboost.csv",sep=",",row.names=FALSE)
# Final submissions
dataset <- read.csv2("data/test.csv",sep=",", stringsAsFactors = FALSE)
# Functions for data prep
source("encodeFeatures.R")
source("performance.R")
# Encode data as dummies and numeric
names(dataset) <- toupper(names(dataset))
dataset <- encodeFeatures(dataset)
# All the numerical features from the spreadsheet. All usage based
numerical_features <-c("ADJMOU","ADJQTY","ADJREV","ATTEMPT_MEAN","ATTEMPT_RANGE","AVG3MOU","AVG3QTY","AVG3REV","AVG6MOU","AVG6QTY","AVG6REV","AVGMOU","AVGQTY","AVGREV","BLCK_DAT_MEAN","BLCK_DAT_RANGE","BLCK_VCE_MEAN","BLCK_VCE_RANGE","CALLFWDV_MEAN","CALLFWDV_RANGE","CALLWAIT_MEAN","CALLWAIT_RANGE","CC_MOU_MEAN","CC_MOU_RANGE","CCRNDMOU_MEAN","CCRNDMOU_RANGE","CHANGE_MOU","CHANGE_REV","COMP_DAT_MEAN","COMP_DAT_RANGE","COMP_VCE_MEAN","COMP_VCE_RANGE","COMPLETE_MEAN","COMPLETE_RANGE","CUSTCARE_MEAN","CUSTCARE_RANGE","DA_MEAN","DA_RANGE","DATOVR_MEAN","DATOVR_RANGE","DROP_BLK_MEAN","DROP_BLK_RANGE","DROP_DAT_MEAN","DROP_DAT_RANGE","DROP_VCE_MEAN","DROP_VCE_RANGE","EQPDAYS","INONEMIN_MEAN","INONEMIN_RANGE","IWYLIS_VCE_MEAN","IWYLIS_VCE_RANGE","MONTHS","MOU_CDAT_MEAN","MOU_CDAT_RANGE","MOU_CVCE_MEAN","MOU_CVCE_RANGE","MOU_MEAN","MOU_OPKD_MEAN","MOU_OPKD_RANGE","MOU_OPKV_MEAN","MOU_OPKV_RANGE","MOU_PEAD_MEAN","MOU_PEAD_RANGE","MOU_PEAV_MEAN","MOU_PEAV_RANGE","MOU_RANGE","MOU_RVCE_MEAN","MOU_RVCE_RANGE","MOUIWYLISV_MEAN","MOUIWYLISV_RANGE","MOUOWYLISV_MEAN","MOUOWYLISV_RANGE","OWYLIS_VCE_MEAN","OWYLIS_VCE_RANGE","OPK_DAT_MEAN","OPK_DAT_RANGE","OPK_VCE_MEAN","OPK_VCE_RANGE","OVRMOU_MEAN","OVRMOU_RANGE","OVRREV_MEAN","OVRREV_RANGE","PEAK_DAT_MEAN","PEAK_DAT_RANGE","PEAK_VCE_MEAN","PEAK_VCE_RANGE","PLCD_DAT_MEAN","PLCD_DAT_RANGE","PLCD_VCE_MEAN","PLCD_VCE_RANGE","RECV_SMS_MEAN","RECV_SMS_RANGE","RECV_VCE_MEAN","RECV_VCE_RANGE","RETDAYS","REV_MEAN","REV_RANGE","RMCALLS","RMMOU","RMREV","ROAM_MEAN","ROAM_RANGE","THREEWAY_MEAN","THREEWAY_RANGE","TOTCALLS","TOTMOU","TOTMRC_MEAN","TOTMRC_RANGE","TOTREV","UNAN_DAT_MEAN","UNAN_DAT_RANGE","UNAN_VCE_MEAN","UNAN_VCE_RANGE","VCEOVR_MEAN","VCEOVR_RANGE")
# impute the data
source('imputeData.R')
#select features
source("selectFeatures.R")
pred.xgb.prob <- predict(xgbFit,dataset.final, type="prob")["leave"]
performance(pred.xgb.prob,dataset.final$churn)
x <- data.frame(Customer_ID=ids,EstimatedChurnProbability=pred.xgb.prob$leave)
write.table(x,"results/final_submissions.csv",sep=",",row.names=FALSE)
dataset <- read.csv2("data/test.csv",sep=",", stringsAsFactors = FALSE)
# Functions for data prep
source("encodeFeatures.R")
source("performance.R")
# Encode data as dummies and numeric
names(dataset) <- toupper(names(dataset))
dataset <- encodeFeatures(dataset)
x <- dataset
numerical_features <-c("ADJMOU","ADJQTY","ADJREV","ATTEMPT_MEAN","ATTEMPT_RANGE","AVG3MOU","AVG3QTY","AVG3REV","AVG6MOU","AVG6QTY","AVG6REV","AVGMOU","AVGQTY","AVGREV","BLCK_DAT_MEAN","BLCK_DAT_RANGE","BLCK_VCE_MEAN","BLCK_VCE_RANGE","CALLFWDV_MEAN","CALLFWDV_RANGE","CALLWAIT_MEAN","CALLWAIT_RANGE","CC_MOU_MEAN","CC_MOU_RANGE","CCRNDMOU_MEAN","CCRNDMOU_RANGE","CHANGE_MOU","CHANGE_REV","COMP_DAT_MEAN","COMP_DAT_RANGE","COMP_VCE_MEAN","COMP_VCE_RANGE","COMPLETE_MEAN","COMPLETE_RANGE","CUSTCARE_MEAN","CUSTCARE_RANGE","DA_MEAN","DA_RANGE","DATOVR_MEAN","DATOVR_RANGE","DROP_BLK_MEAN","DROP_BLK_RANGE","DROP_DAT_MEAN","DROP_DAT_RANGE","DROP_VCE_MEAN","DROP_VCE_RANGE","EQPDAYS","INONEMIN_MEAN","INONEMIN_RANGE","IWYLIS_VCE_MEAN","IWYLIS_VCE_RANGE","MONTHS","MOU_CDAT_MEAN","MOU_CDAT_RANGE","MOU_CVCE_MEAN","MOU_CVCE_RANGE","MOU_MEAN","MOU_OPKD_MEAN","MOU_OPKD_RANGE","MOU_OPKV_MEAN","MOU_OPKV_RANGE","MOU_PEAD_MEAN","MOU_PEAD_RANGE","MOU_PEAV_MEAN","MOU_PEAV_RANGE","MOU_RANGE","MOU_RVCE_MEAN","MOU_RVCE_RANGE","MOUIWYLISV_MEAN","MOUIWYLISV_RANGE","MOUOWYLISV_MEAN","MOUOWYLISV_RANGE","OWYLIS_VCE_MEAN","OWYLIS_VCE_RANGE","OPK_DAT_MEAN","OPK_DAT_RANGE","OPK_VCE_MEAN","OPK_VCE_RANGE","OVRMOU_MEAN","OVRMOU_RANGE","OVRREV_MEAN","OVRREV_RANGE","PEAK_DAT_MEAN","PEAK_DAT_RANGE","PEAK_VCE_MEAN","PEAK_VCE_RANGE","PLCD_DAT_MEAN","PLCD_DAT_RANGE","PLCD_VCE_MEAN","PLCD_VCE_RANGE","RECV_SMS_MEAN","RECV_SMS_RANGE","RECV_VCE_MEAN","RECV_VCE_RANGE","RETDAYS","REV_MEAN","REV_RANGE","RMCALLS","RMMOU","RMREV","ROAM_MEAN","ROAM_RANGE","THREEWAY_MEAN","THREEWAY_RANGE","TOTCALLS","TOTMOU","TOTMRC_MEAN","TOTMRC_RANGE","TOTREV","UNAN_DAT_MEAN","UNAN_DAT_RANGE","UNAN_VCE_MEAN","UNAN_VCE_RANGE","VCEOVR_MEAN","VCEOVR_RANGE")
# All the categorical features from the spreadsheet. Most of them will be transformed into dummy variables, numerical variables or dropped
categorical_features <- c("ACTVSUBS","ADULTS","AGE1","AGE2","AREA","ASL_FLAG","CAR_BUY","CARTYPE","CHILDREN","CHURN","CRCLSCOD","CREDITCD","CRTCOUNT","CSA","CUSTOMER_ID","DIV_TYPE","DUALBAND","DWLLSIZE","DWLLTYPE","EDUC1","ETHNIC","FORGNTVL","HND_PRICE","HHSTATIN","HND_WEBCAP","INCOME","INFOBASE","KID0_2","KID3_5","KID6_10","KID11_15","KID16_17","LAST_SWAP","LOR","MAILFLAG","MAILORDR","MAILRESP","MARITAL","MODELS","MTRCYCLE","NEW_CELL","NUMBCARS","OCCU1","OWNRENT","PCOWNER","PHONES","PRE_HND_PRICE","PRIZM_SOCIAL_ONE","PROPTYPE","REF_QTY","REFURB_NEW","RV","SOLFLAG","TOT_ACPT","TOT_RET","TRUCK","UNIQSUBS","WRKWOMAN")
# Encode all numerical features as such
x[,names(x) %in% numerical_features] <- apply(x[,names(x) %in% numerical_features],2,as.numeric)
# Save the categorical features seperately and delete from dataframe
cats <- x[,names(x) %in% categorical_features]
x <- x[,!names(x) %in% categorical_features]
# Encode active subs as numerical
cats$ACTIVESUBS <- as.numeric(cats$ACTVSUBS)
# Dummyfy spending limit
cats$SPENDING_LIMIT <- 0
cats[cats$ASL_FLAG=="Y",]$SPENDING_LIMIT <- 1
# Dummyfy new car
cats$NEW_CAR<- 0
cats[cats$CAR_BUY=="New",]$NEW_CAR <- 1
# Dummyfy credit class
cats$GOOD_CREDIT_SCORE <- 0
cats[cats$CRCLSCOD  %in% c("A","A2","AA","B","B2","BA"),]$GOOD_CREDIT_SCORE <- 1
cats$BAD_CREDIT_SCORE <- 0
cats[cats$CRCLSCOD  %in% c("Z1","Z2","Z3","Z4","Z5","ZA","ZB","ZF","ZY","ZZ"),]$BAD_CREDIT_SCORE <- 1
# Dummyfy credit card indicator
cats$HAS_CREDIT_CARD <- 0
cats[cats$CREDITCD=="Y",]$HAS_CREDIT_CARD <- 1
# make numerical out of adjustment made to credit rating
cats[is.na(cats$CRTCOUNT),]$CRTCOUNT <- 0
cats$CREDIT_ADJUSTED <- as.numeric(cats$CRTCOUNT)
# Dummyfy additional services
cats$LONG_DISTANCE_SERVICE <- 0
cats$LOCAL_PHONE_SERVICE <- 0
cats[cats$DIV_TYPE=="LDD",]$LONG_DISTANCE_SERVICE <- 1
cats[cats$DIV_TYPE=="BTH",]$LONG_DISTANCE_SERVICE <- 1
cats[cats$DIV_TYPE=="LTD",]$LOCAL_PHONE_SERVICE <- 1
cats[cats$DIV_TYPE=="BTH",]$LOCAL_PHONE_SERVICE <- 1
# Dummyfy dualband
cats$DB <- 0
cats[cats$DUALBAND=="Y",]$DB <- 1
cats[cats$DUALBAND=="T",]$DB <- 1
# dummyfy foreign travel
cats$FOREIGN_TRAVEL <- ifelse(!is.na(cats$FORGNTVL) & cats$FORGNTVL==1 , 1 , 0)
# Convert current handset price to numerical
cats$CURRENT_PHONE_PRICE <- as.numeric(cats$HND_PRICE)
cats[is.na(cats$CURRENT_PHONE_PRICE),]$CURRENT_PHONE_PRICE <- median(cats$CURRENT_PHONE_PRICE, na.rm = TRUE)
# Dummy db information about customer
cats$FULL_INFORMATION <- 0
cats[cats$HHSTATIN=="C",]$FULL_INFORMATION <- 1
cats[cats$HHSTATIN=="I",]$FULL_INFORMATION <- 1
# Convert estimated income to numerical
cats$ESTIMATED_INCOME <- as.numeric(cats$INCOME)
cats[is.na(cats$ESTIMATED_INCOME),]$ESTIMATED_INCOME<- median(cats$ESTIMATED_INCOME, na.rm = TRUE)
# Get the years of the last swapped variable
substrRight <- function(x, n){
substr(x, nchar(x)-n+1, nchar(x))
}
years <- substrRight(cats$LAST_SWAP,4)
cats$LAST_PHONE_SWAP = 0
cats[years=="1999",]$LAST_PHONE_SWAP<- 2
cats[years=="2000",]$LAST_PHONE_SWAP<- 1
# dummyfy do not mail flag
cats$DO_NOT_MAIL <- 0
cats[cats$MAILFLAG=="Y",]$DO_NOT_MAIL <- 1
# dummyfy mail responder
cats$RESPOND_TO_MAIL <- 0
cats[cats$MAILRESP=="R",]$RESPOND_TO_MAIL <- 1
# Make number of models numeric
cats$MODELS_ISSUED <- as.numeric(cats$MODELS)
# make binary for mobile home owners
cats$OWNS_MOBILE_HOME <- 0
cats[cats$PROPTYPE=="M",]$OWNS_MOBILE_HOME <- 1
# make number of referrals numeric
cats[is.na(cats$REF_QTY),]$REF_QTY <- 0
cats$REF_QTY_NUM <- as.numeric(cats$REF_QTY)
# dummyfy refurbished phone
cats$REFURBISHED <- 0
cats[cats$REFURB_NEW=="R",]$REFURBISHED <- 1
# dummyfy RV
cats$OWNS_RV <- ifelse(!is.na(cats$RV) & cats$RV==1 , 1 , 0)
# dummyfy do not solicit
cats$DO_NOT_SOLICIT <- 0
cats[cats$SOLFLAG=="N",]$DO_NOT_SOLICIT <- 1
# encode if they have received and declined offers from retention team
cats[is.na(cats$TOT_ACPT),]$TOT_ACPT <- 99
cats$TOT_ACPT <- as.numeric(cats$TOT_ACPT)
cats$RECEIVED_OFFER <- 1
cats[cats$TOT_ACPT==99,]$RECEIVED_OFFER <- 0
cats$DECLINED_OFFER <- 0
cats[cats$TOT_ACPT==0,]$DECLINED_OFFER <- 1
cats$NUMBER_OF_OFFERS <- 0
cats$NUMBER_OF_OFFERS <- ifelse(cats$RECEIVED_OFFER==1,cats$TOT_ACPT,0)
# make numerical the number of retention calls they have received
cats$NUMBER_OF_RETENTION_CALLS <- as.numeric(cats$TOT_RET)
cats[is.na(cats$TOT_RET),]$NUMBER_OF_RETENTION_CALLS <- 0
# make numerical the number of subs they have
cats$NUMBER_OF_SUBS <- as.numeric(cats$UNIQSUBS)
churn <- cats$CHURN
churn <- factor(churn,levels=c(0,1),labels=c("stay","leave"))
id <- cats$CUSTOMER_ID
cats <- cats[,!names(cats) %in% categorical_features]
dataset <- cbind(x,cats,churn,id)
d <- cbind(x,cats,churn,id)
dataset <- cbind(x,catsid)
dataset <- cbind(x,cats,id)
source('imputeData.R')
#select features
source("selectFeatures.R")
pred.xgb.prob <- predict(xgbFit,dataset.final, type="prob")["leave"]
x <- data.frame(Customer_ID=ids,EstimatedChurnProbability=pred.xgb.prob$leave)
write.table(x,"results/final_submissions.csv",sep=",",row.names=FALSE)
plot(varImp(xgbFit))
